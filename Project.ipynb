{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 04:37:23.821467: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-09 04:37:24.179702: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746754644.355922   51234 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746754644.395864   51234 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746754644.719224   51234 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746754644.719292   51234 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746754644.719298   51234 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746754644.719302   51234 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-09 04:37:24.766700: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from conllu import parse_incr\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pyconll\n",
    "# import tensorflow as tf \n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "from transformers import create_optimizer\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".* seems not to be NE tag.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "EPOCHS = 2\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## login huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenList<برلين, ترفض, حصول, شركة, اميركية, على, رخصة, تصنيع, دبابة, \", ليوبارد, \", الالمانية, metadata={newdoc id: \"afp.20000715.0075\", newpar id: \"afp.20000715.0075:p1\", sent_id: \"afp.20000715.0075:p1u1\", text: \"برلين ترفض حصول شركة اميركية على رخصة تصنيع دبابة \"ليوبارد\" الالمانية\"}>\n",
      "TokenList<برلين, 15, -, 7, (, اف, ب, ), -, افادت, صحيفة, الاحد, الالمانية, \", ويلت, ام, سونتاغ, \", في, عددها, عدد, ها, الصادر, غدا, ،, ان, المستشار, غيرهارد, شرودر, يرفض, حصول, المجموعة, الاميركية, \", جنرال, ديناميكس, \", على, رخصة, لتصنيع, ل, تصنيع, الدبابة, الالمانية, \", ليوبارد, 2, \", عبر, شراء, المجموعة, الحكومية, الاسبانية, للاسلحة, ل, الأسلحة, \", سانتا, بربارة, \", ., metadata={newpar id: \"afp.20000715.0075:p2\", sent_id: \"afp.20000715.0075:p2u1\", text: \"برلين 15-7 (اف ب) - افادت صحيفة الاحد الالمانية \"ويلت ام سونتاغ\" في عددها الصادر غدا، ان المستشار غيرهارد شرودر يرفض حصول المجموعة الاميركية \"جنرال ديناميكس\" على رخصة لتصنيع الدبابة الالمانية \"ليوبارد 2\" عبر شراء المجموعة الحكومية الاسبانية للاسلحة \"سانتا بربارة\".\"}>\n",
      "TokenList<وفي, و, في, نيسان, /, ابريل, الماضي, ،, تخلت, الدولة, الاسبانية, عن, مجموعة, \", سانتا, بربارة, \", التي, تصنع, دبابات, ليوبارد, الالمانية, ،, الى, \", جنرال, ديناميكس, \", التي, تنتج, الدبابة, الاميركية, \", ام, 1, ابرامس, \", المعتبرة, المنافسة, الرئيسية, لدبابة, ل, دبابة, ليوبارد, في, الاسواق, ., metadata={newpar id: \"afp.20000715.0075:p3\", sent_id: \"afp.20000715.0075:p3u1\", text: \"وفي نيسان/ابريل الماضي، تخلت الدولة الاسبانية عن مجموعة \"سانتا بربارة\" التي تصنع دبابات ليوبارد الالمانية، الى \"جنرال ديناميكس\" التي تنتج الدبابة الاميركية \"ام1 ابرامس\" المعتبرة المنافسة الرئيسية لدبابة ليوبارد في الاسواق.\"}>\n",
      "TokenList<وكانت, و, كانت, خسائر, المجموعة, الاسبانية, الرسمية, تراكمت, في, السنوات, العشر, الاخيرة, لتبلغ, ل, تبلغ, 920, مليون, يورو, ., metadata={newpar id: \"afp.20000715.0075:p4\", sent_id: \"afp.20000715.0075:p4u1\", text: \"وكانت خسائر المجموعة الاسبانية الرسمية تراكمت في السنوات العشر الاخيرة لتبلغ 920 مليون يورو.\"}>\n",
      "TokenList<واشارت, و, أشارت, صحيفة, الاحد, الى, ان, المستشار, شرودر, يعتبر, ان, حصول, المجموعة, الاميركية, على, رخصة, تصنيع, ليوبارد, 2, يعرض, للخطر, ل, الخطر, المصالح, التكنولوجية, الاساسية, لالمانيا, ل, ألمانيا, وطلب, و, طلب, من, وزير, الدفاع, رودولف, شاربينغ, التوجه, الى, اسبانيا, ., metadata={newpar id: \"afp.20000715.0075:p5\", sent_id: \"afp.20000715.0075:p5u1\", text: \"واشارت صحيفة الاحد الى ان المستشار شرودر يعتبر ان حصول المجموعة الاميركية على رخصة تصنيع ليوبارد2 يعرض للخطر المصالح التكنولوجية الاساسية لالمانيا وطلب من وزير الدفاع رودولف شاربينغ التوجه الى اسبانيا.\"}>\n",
      "TokenList<وتعذر, و, تعذر, لمتحدثة, ل, متحدثة, باسم, ب, اسم, وزارة, الدفاع, الالمانية, ان, تؤكد, اليوم, السبت, هذه, المعلومات, ., metadata={newpar id: \"afp.20000715.0075:p6\", sent_id: \"afp.20000715.0075:p6u1\", text: \"وتعذر لمتحدثة باسم وزارة الدفاع الالمانية ان تؤكد اليوم السبت هذه المعلومات.\"}>\n",
      "TokenList<وفي, و, في, 21, حزيران, /, يونيو, ،, شدد, وزير, الدفاع, الاسباني, فيديريكو, تريو, على, ان, العقد, الموقع, في, نهاية, عام, 1998, بين, سانتا, بربارة, ومنتجي, و, منتجي, الدبابة, الالمانية, ،, كروس, ك, روس, مافييه, ورينمينتال, لتصنيع, ل, تصنيع, الدبابة, في, اسبانيا, وانتاج, و, إنتاج, 235, دبابة, ليوبارد, 2, خلال, عشر, سنوات, ،, يتضمن, بندا, لحماية, ل, حماية, التكنولوجيا, الالمانية, ويجب, و, يجب, في, \", جميع, الاحوال, \", ان, تحترمه, تحترم, ه, شركة, \", جنرال, ديناميكس, \", الاميركية, ., metadata={newpar id: \"afp.20000715.0075:p7\", sent_id: \"afp.20000715.0075:p7u1\", text: \"وفي 21 حزيران/يونيو، شدد وزير الدفاع الاسباني فيديريكو تريو على ان العقد الموقع في نهاية عام 1998 بين سانتا بربارة ومنتجي الدبابة الالمانية، كروس مافييه ورينمينتال لتصنيع الدبابة في اسبانيا وانتاج 235 دبابة ليوبارد2 خلال عشر سنوات، يتضمن بندا لحماية التكنولوجيا الالمانية ويجب في \"جميع الاحوال\" ان تحترمه شركة \"جنرال ديناميكس\" الاميركية.\"}>\n",
      "TokenList<مقتل, ما, لا, يقل, عن, 12, شخصا, في, مواجهات, في, احدى, اسواق, لاغوس, metadata={newdoc id: \"afp.20000715.0076\", newpar id: \"afp.20000715.0076:p1\", sent_id: \"afp.20000715.0076:p1u1\", text: \"مقتل ما لا يقل عن 12 شخصا في مواجهات في احدى اسواق لاغوس\"}>\n",
      "TokenList<لاغوس, 15, -, 7, (, اف, ب, ), -, ذكرت, الشرطة, النيجيرية, اليوم, السبت, ان, ما, لا, يقل, عن, 12, شخصا, قتلوا, في, مواجهات, استمرت, يومين, في, سوق, في, لاغوس, بين, تجار, وزقاقيين, و, زقاقيين, ., metadata={newpar id: \"afp.20000715.0076:p2\", sent_id: \"afp.20000715.0076:p2u1\", text: \"لاغوس 15-7 (اف ب) - ذكرت الشرطة النيجيرية اليوم السبت ان ما لا يقل عن 12 شخصا قتلوا في مواجهات استمرت يومين في سوق في لاغوس بين تجار وزقاقيين.\"}>\n",
      "TokenList<واعلن, و, أعلن, متحدث, باسم, ب, اسم, الشرطة, لوكالة, ل, وكالة, فرانس, برس, ان, المواجهات, التي, بدأت, الخميس, بين, التجار, في, سوق, الابا, في, لاغوس, ،, وعدد, و, عدد, من, الزقاقيين, ،, اسفرت, ايضا, عن, اصابة, عدد, من, الاشخاص, بجروح, ب, جروح, ., metadata={newpar id: \"afp.20000715.0076:p3\", sent_id: \"afp.20000715.0076:p3u1\", text: \"واعلن متحدث باسم الشرطة لوكالة فرانس برس ان المواجهات التي بدأت الخميس بين التجار في سوق الابا في لاغوس، وعدد من الزقاقيين، اسفرت ايضا عن اصابة عدد من الاشخاص بجروح.\"}>\n"
     ]
    }
   ],
   "source": [
    "with open(\"Arabic_POS.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    count = 0\n",
    "    for tokenlist in parse_incr(f):\n",
    "        print(tokenlist)\n",
    "        count += 1\n",
    "        if count == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pyconll.load_from_file('Arabic_POS.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "for sentence in data:\n",
    "  sentences.append([token.form for token in sentence])\n",
    "  labels.append([token.upos for token in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DET', 'SCONJ', 'X', 'CCONJ', 'PART', 'ADV', 'PROPN', 'VERB', 'AUX', 'PRON', 'PUNCT', 'NUM', 'INTJ', 'ADP', 'NOUN', 'ADJ', 'SYM'}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set(label for sublist in labels for label in sublist if label is not None)\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = ['CCONJ', 'VERB', 'ADP', 'DET', 'NUM', 'PRON', 'AUX', 'PROPN', 'PUNCT', 'PART', 'NOUN', 'SYM', 'SCONJ', 'INTJ', 'ADV', 'ADJ', 'X', 'None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X',\n",
       " 'NUM',\n",
       " 'PUNCT',\n",
       " 'NUM',\n",
       " 'PUNCT',\n",
       " 'X',\n",
       " 'X',\n",
       " 'PUNCT',\n",
       " 'PUNCT',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'PUNCT',\n",
       " 'X',\n",
       " 'X',\n",
       " 'X',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " None,\n",
       " 'NOUN',\n",
       " 'PRON',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'NOUN',\n",
       " 'X',\n",
       " 'X',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'PUNCT',\n",
       " 'X',\n",
       " 'X',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " None,\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'PUNCT',\n",
       " 'X',\n",
       " 'NUM',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'ADJ',\n",
       " None,\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'X',\n",
       " 'X',\n",
       " 'PUNCT',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['برلين',\n",
       " '15',\n",
       " '-',\n",
       " '7',\n",
       " '(',\n",
       " 'اف',\n",
       " 'ب',\n",
       " ')',\n",
       " '-',\n",
       " 'افادت',\n",
       " 'صحيفة',\n",
       " 'الاحد',\n",
       " 'الالمانية',\n",
       " '\"',\n",
       " 'ويلت',\n",
       " 'ام',\n",
       " 'سونتاغ',\n",
       " '\"',\n",
       " 'في',\n",
       " 'عددها',\n",
       " 'عدد',\n",
       " 'ها',\n",
       " 'الصادر',\n",
       " 'غدا',\n",
       " '،',\n",
       " 'ان',\n",
       " 'المستشار',\n",
       " 'غيرهارد',\n",
       " 'شرودر',\n",
       " 'يرفض',\n",
       " 'حصول',\n",
       " 'المجموعة',\n",
       " 'الاميركية',\n",
       " '\"',\n",
       " 'جنرال',\n",
       " 'ديناميكس',\n",
       " '\"',\n",
       " 'على',\n",
       " 'رخصة',\n",
       " 'لتصنيع',\n",
       " 'ل',\n",
       " 'تصنيع',\n",
       " 'الدبابة',\n",
       " 'الالمانية',\n",
       " '\"',\n",
       " 'ليوبارد',\n",
       " '2',\n",
       " '\"',\n",
       " 'عبر',\n",
       " 'شراء',\n",
       " 'المجموعة',\n",
       " 'الحكومية',\n",
       " 'الاسبانية',\n",
       " 'للاسلحة',\n",
       " 'ل',\n",
       " 'الأسلحة',\n",
       " '\"',\n",
       " 'سانتا',\n",
       " 'بربارة',\n",
       " '\"',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentense_id = []\n",
    "for i in range (len(sentences)):\n",
    "  sentense_id.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentense_id[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 1, 10, 10, 15, 2, 10, 10, 10, 8, 16, 8, 15]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {i: l for i, l in enumerate(labels_list)}\n",
    "\n",
    "# Ensure all 'None' values are replaced with a valid key\n",
    "labels = [\n",
    "    [tag if tag is not None else \"None\" for tag in tag_sequence]\n",
    "    for tag_sequence in labels\n",
    "]\n",
    "\n",
    "# Build label_ids using the updated labels\n",
    "label_ids = [\n",
    "    [label_map[tag] for tag in tag_sequence]\n",
    "    for tag_sequence in labels\n",
    "]\n",
    "label_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'X',\n",
       " 'PUNCT',\n",
       " 'ADJ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'id': sentense_id,\n",
    "    'ner_tags': label_ids,\n",
    "    'tokens': sentences,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize tokenizer with correct Arabic model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"aubmindlab/bert-base-arabertv02\",\n",
    "    max_length=128,  # Add max length\n",
    "    truncation=True\n",
    ")\n",
    "# 2. Now create the data collator\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors=\"tf\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c2cab3864e4a6197574012db9beeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6075 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data_dict.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = tokenized_data.train_test_split(test_size=0.2, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_train = train_test_split['train']\n",
    "tokenized_data_test = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "num_train_steps = (len(train_test_split[\"train\"]) // BATCH_SIZE) * EPOCHS\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746754687.650913   51234 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4269 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    \"aubmindlab/bert-base-arabertv02\", \n",
    "    num_labels=len(labels_list),\n",
    "    id2label=id2label, \n",
    "    label2id=label_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    train_test_split[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    train_test_split[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/israa/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/home/israa/Desktop/Arabic_POS_Tagging/Arabic_POS_model is already a clone of https://huggingface.co/Israaabdelghany/Arabic_POS. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"Arabic_POS_classifer\",\n",
    "    hub_model_id=\"Israaabdelghany/Arabic_POS_classifer\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "callbacks = [metric_callback, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2430/2430 [==============================] - 587s 241ms/step - loss: 0.0670 - val_loss: 0.1015 - precision: 0.9651 - recall: 0.9605 - f1: 0.9628 - accuracy: 0.9725\n",
      "Epoch 2/2\n",
      "2430/2430 [==============================] - ETA: 0s - loss: 0.0665"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 05:43:16.624128: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2430/2430 [==============================] - 601s 247ms/step - loss: 0.0665 - val_loss: 0.1015 - precision: 0.9651 - recall: 0.9605 - f1: 0.9628 - accuracy: 0.9725\n"
     ]
    }
   ],
   "source": [
    "# Reduce batch size to avoid OOM errors\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=EPOCHS, callbacks=callbacks, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"Arabic_POS_classifer\", id2label=id2label, label2id=label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"Arabic_POS_classifer\")\n",
    "tokenizer.push_to_hub(\"Arabic_POS_classifer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"برلين ترفض حصول شركة امريكية على رخصة\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\", model=\"MariamOsama3/Mariam_classifer2\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
